2023-10-04 12:33:38| file for running: E:\Project_python\ml-design-hw\scr\train.py
2023-10-04 12:33:38| import os
import yaml
import time
import datetime
import warnings
from tqdm import tqdm

import torch
import timm
import pandas as pd
import numpy as np
import torch.nn as nn

from box import Box
from torch.utils.data import DataLoader
from loguru import logger
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR

from utils.create_dataset import BirdDataset
from utils.base_utils import set_seed
from utils.metrics import validation_epoch_end

warnings.filterwarnings("ignore", category=UserWarning)
date_now = datetime.datetime.now().strftime("%d_%B_%Y_%H_%M")



def main():
    for epoch_i in range(1, config.epochs + 1):
        if config.debug:
            k = 1
        start = time.time()
        logger.info(f'---------------------epoch:{epoch_i}/{config.epochs}---------------------')

        # loss
        avg_train_loss = 0
        avg_val_loss = 0
        predicted_labels_list = None
        true_labels_list = None

        ############## Train #############
        model.train()
        train_pbar = tqdm(train_loader, desc="Training")
        for batch in train_pbar:
            X_batch = batch[0].to(config.device)
            y_batch = batch[1].to(config.device)

            optimizer.zero_grad()
            res = model.forward(X_batch)
            loss = loss_f(res.float(), y_batch)

            if torch.cuda.is_available():
                train_pbar.set_postfix(gpu_load=f"{torch.cuda.memory_allocated() / 1024 ** 3:.2f}GB",
                                       loss=f"{loss.item():.4f}")
            else:
                train_pbar.set_postfix(loss=f"{loss.item():.4f}")

            loss.backward()
            optimizer.step()

            avg_train_loss += loss * len(y_batch)
            del batch, res

            if config.scheduler:
                scheduler.step()

            if config.debug:
                k += 1
                if k > 5:
                    break

        model.eval()

        ########## VALIDATION ###############
        with torch.no_grad():
            for batch in (valid_loader):
                X_batch = batch[0].to(config.device)
                y_batch = batch[1].to(config.device)

                res = model.forward(X_batch)
                loss = loss_f(res.float(), y_batch)
                y_batch_onehot = y_batch

                avg_val_loss += loss * len(y_batch)

                # metrics
                res = res.detach().cpu().sigmoid().numpy()
                y_batch_onehot = y_batch_onehot.unsqueeze(1).detach().cpu().numpy()
                y_batch_onehot = y_batch_onehot.squeeze()

                if predicted_labels_list is None:
                    predicted_labels_list = res
                    true_labels_list = y_batch_onehot
                else:
                    predicted_labels_list = np.concatenate([predicted_labels_list, res], axis=0)
                    true_labels_list = np.concatenate([true_labels_list, y_batch_onehot], axis=0)

                del batch, res

                if config.debug:
                    k += 1
                    if k > 10:
                        break

        torch.cuda.empty_cache()

        avg_train_loss = avg_train_loss / len(dataset_train)
        avg_val_loss = avg_val_loss / len(dataset_test)

        all_predicted_labels = np.vstack(predicted_labels_list)
        all_true_labels = np.vstack(true_labels_list)
        all_true_labels = np.squeeze(all_true_labels)
        mask = (all_true_labels > 0) & (all_true_labels < 1)
        all_true_labels[mask] = 0
        avg_metric = metric(all_true_labels, all_predicted_labels)

        logger.info(f'epoch: {epoch_i}')

        logger.info("loss_train: %0.4f| loss_valid: %0.4f|" % (avg_train_loss, avg_val_loss))
        for m in avg_metric:
            logger.info(f"metric {m} : {avg_metric[m]:.<5g}")

        elapsed_time = time.time() - start
        hours = int(elapsed_time // 3600)
        minutes = int((elapsed_time % 3600) // 60)
        seconds = int(elapsed_time % 60)
        logger.info(f"Elapsed time: {hours:02d}:{minutes:02d}:{seconds:02d}")

        if epoch_i % 4 == 0:
            torch.save(model, f'{path_save}/model_{config.model_name}_ep_{epoch_i}.pt')

        torch.save(model, f'{path_save}/model_{config.model_name}_last_version.pt')


if __name__  == "__main__":
    path_save = os.path.join("../experiment", date_now)
    if not os.path.exists(path_save):
        os.makedirs(path_save)

    # Load config with params for training
    with open("config.yaml", "r") as f:
        config = yaml.load(f, Loader=yaml.SafeLoader)
        config = Box(config)

    logger.add(f"{path_save}/info_log{date_now}.log",
               format="<red>{time:YYYY-MM-DD HH:mm:ss}</red>| {message}")

    file_name = __file__
    logger.info(f'file for running: {file_name}')
    with open(file_name, 'r') as file:
        code = file.read()
        logger.info(code)

    logger.info(f"Folder with experiment- {path_save}")
    logger.info("----------params----------")

    for param in config:
        logger.info(f"{param}: {str(config[param])}")

    # Create device for training and set_seed:
    config.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    set_seed(seed=config.seed)

    # Read data
    df = pd.read_csv("../data/data.csv")
    df_train, df_test = (df[df.fold != 3].reset_index(drop=True),
                         df[df.fold == 3].reset_index(drop=True)
                         )

    logger.info(f"Size df_train- {df_train.shape[0]}")
    logger.info(f"Size df_test- {df_test.shape[0]}")

    dataset_train = BirdDataset(df=df_train,
                                path_to_folder_with_audio=config.path_to_files_base
                                )
    dataset_test = BirdDataset(df=df_test,
                               path_to_folder_with_audio=config.path_to_files_base
                               )

    train_loader = DataLoader(dataset_train,
                              batch_size=config.batch_size,
                              shuffle=True,
                              num_workers=config.num_workers)
    valid_loader = DataLoader(dataset_test,
                              batch_size=config.batch_size,
                              num_workers=config.num_workers)

    logger.info(f"New experiment")
    model_name = config.model_name
    model = timm.create_model(model_name, pretrained=True).to(config.device)
    model.classifier = nn.Sequential(
        nn.Linear(model.classifier.in_features, 264)
    )
    if torch.cuda.device_count() > 1:
        print("Let's use", torch.cuda.device_count(), "GPUs!")
        model = nn.DataParallel(model)
    model.to(config.device)

    if config.metric == 'custom':
        metric = validation_epoch_end

    if config.loss_f == "nn.BCEWithLogitsLoss()":
        loss_f = nn.BCEWithLogitsLoss()

    # optimizer
    if config.optimizer == "Adam":
        optimizer = torch.optim.Adam(model.parameters(),
                                     lr=config.optimizer_lr,
                                     weight_decay=config.optimizer_wd
                                     )

    if config.scheduler == "CosineAnnealingWarmRestarts":
        logger.info(f"Scheduler - {config.scheduler}")
        scheduler = CosineAnnealingWarmRestarts(optimizer,
                                                T_0=10,
                                                T_mult=2,
                                                eta_min=0.000001,
                                                last_epoch=-1)

    # Train_loop
    logger.info(f"Starting train. Model - {config.model_name}")

    main()

2023-10-04 12:33:38| Folder with experiment- ../experiment\04_October_2023_12_33
2023-10-04 12:33:38| ----------params----------
2023-10-04 12:33:38| debug: False
2023-10-04 12:33:38| seed: 1771
2023-10-04 12:33:38| path_to_files_base: ../data
2023-10-04 12:33:38| batch_size: 16
2023-10-04 12:33:38| num_workers: 0
2023-10-04 12:33:38| optimizer_lr: 0.006
2023-10-04 12:33:38| optimizer_wd: 0
2023-10-04 12:33:38| scheduler: CosineAnnealingWarmRestarts
2023-10-04 12:33:38| model_name: tf_efficientnet_b2
2023-10-04 12:33:38| metric: custom
2023-10-04 12:33:38| loss_f: nn.BCEWithLogitsLoss()
2023-10-04 12:33:38| optimizer: Adam
2023-10-04 12:33:38| epochs: 5
2023-10-04 12:33:38| Set seed: 1771
2023-10-04 12:33:38| Size df_train- 12326
2023-10-04 12:33:38| Size df_test- 3082
2023-10-04 12:33:38| New experiment
2023-10-04 12:33:39| Scheduler - CosineAnnealingWarmRestarts
2023-10-04 12:33:39| Starting train. Model - tf_efficientnet_b2
2023-10-04 12:33:39| ---------------------epoch:1/5---------------------
2023-10-04 13:26:21| epoch: 1
2023-10-04 13:26:21| loss_train: 0.0238| loss_valid: 0.0207|
2023-10-04 13:26:21| metric val_RMAP : 0.220601
2023-10-04 13:26:21| metric CMAP_5 : 0.421362
2023-10-04 13:26:21| Elapsed time: 00:52:41
2023-10-04 13:26:21| ---------------------epoch:2/5---------------------
2023-10-04 14:07:42| epoch: 2
2023-10-04 14:07:42| loss_train: 0.0186| loss_valid: 0.0185|
2023-10-04 14:07:42| metric val_RMAP : 0.337055
2023-10-04 14:07:42| metric CMAP_5 : 0.505424
2023-10-04 14:07:42| Elapsed time: 00:41:20
2023-10-04 14:07:42| ---------------------epoch:3/5---------------------
2023-10-04 14:44:32| epoch: 3
2023-10-04 14:44:32| loss_train: 0.0162| loss_valid: 0.0136|
2023-10-04 14:44:32| metric val_RMAP : 0.499561
2023-10-04 14:44:32| metric CMAP_5 : 0.638994
2023-10-04 14:44:32| Elapsed time: 00:36:50
2023-10-04 14:44:32| ---------------------epoch:4/5---------------------
2023-10-04 15:19:16| epoch: 4
2023-10-04 15:19:16| loss_train: 0.0158| loss_valid: 0.0162|
2023-10-04 15:19:16| metric val_RMAP : 0.413795
2023-10-04 15:19:16| metric CMAP_5 : 0.566994
2023-10-04 15:19:16| Elapsed time: 00:34:43
2023-10-04 15:19:16| ---------------------epoch:5/5---------------------
2023-10-04 15:56:30| epoch: 5
2023-10-04 15:56:30| loss_train: 0.0146| loss_valid: 0.0133|
2023-10-04 15:56:30| metric val_RMAP : 0.532947
2023-10-04 15:56:30| metric CMAP_5 : 0.657892
2023-10-04 15:56:30| Elapsed time: 00:37:14
